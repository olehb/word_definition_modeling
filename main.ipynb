{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import Embeddings, load_glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "400002"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_glove_embeddings(dim)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-9.4531e-01,  3.9686e-01, -8.0605e-01, -3.0215e-01,  2.7736e-01,\n        -1.0019e-01, -4.0500e-01, -1.0095e-01, -6.5934e-02, -4.7258e-02,\n        -2.0828e-01, -2.5721e-01,  6.8750e-02,  9.3751e-01, -8.1483e-02,\n         1.3460e-01,  2.7302e-02, -1.8096e-01, -3.5638e-01, -8.8104e-01,\n         1.1951e+00,  5.5556e-02, -3.1741e-01,  1.0244e+00, -8.4768e-01,\n        -1.5959e+00,  2.1657e-02,  4.3628e-01,  8.8388e-04, -4.1820e-01,\n         2.1247e+00, -4.3332e-01, -1.0816e+00,  3.3616e-01,  3.3399e-01,\n        -2.0064e-01,  5.8633e-01,  9.0186e-02,  7.5054e-01,  4.8500e-01,\n         1.7370e-01,  6.8129e-01, -1.6810e-01,  6.1265e-01,  7.6875e-02,\n        -1.9797e-01, -9.9555e-02, -1.0231e+00,  9.5394e-01, -6.3500e-02])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['bar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"sequencer\", \"dna sequencing was carried out using the dye termination kit and an automatic sequencer\", \"an apparatus for determining the sequence of amino acids or other monomers in a biological polymer\"),\n",
    "    (\"order\", \"the templars were also known as the order of christ\", \"a society of knights bound by a common rule of life and having a combined military and monastic character\"),\n",
    "    (\"horde\", \"an army or tribe of nomadic warriors\", \"the viking hordes returned to york this weekend as fierce armoured warriors mingled with the city centre crowds .\"),\n",
    "    (\"anaemic\", \"although it has been thought of as a symptom of iron deficiency , it is more commonly discovered in patients who are not anemic .\", \"suffering from anaemia\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import (pad_sequence, \n",
    "                                pack_padded_sequence, \n",
    "                                pad_packed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def prepare_batch(strings: List[str]):\n",
    "    sorted_strings = sorted(strings, key=lambda s: len(s), reverse=True)\n",
    "\n",
    "\n",
    "def sentence_to_list(sent):\n",
    "    return [Embeddings.SOS_STR] + sent.split() + [Embeddings.EOS_STR]\n",
    "\n",
    "def strings_to_batch(strings: List[str]) -> torch.nn.utils.rnn.PackedSequence:\n",
    "    sents = sorted([sentence_to_list(sent) for sent in strings], key=lambda s: len(s), reverse=True)\n",
    "    sents_emb = [embeddings.sentence_to_tensor(sent) for sent in sents]\n",
    "    lens = [len(s) for s in sents_emb]\n",
    "    batch = pack_padded_sequence(pad_sequence(sents_emb), lens)\n",
    "    return batch, lens\n",
    "\n",
    "def strings_to_ids(strings: List[str]) -> List[int]:\n",
    "    return [embeddings.sentence_to_ids(sentence_to_list(sent))for sent in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "examples, examples_lens = strings_to_batch([d[1] for d in data])\n",
    "defs, defs_lens = strings_to_batch([d[2] for d in data])\n",
    "defs_ids = strings_to_ids([d[2] for d in data])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wdm import LSTMEncoder, LSTMCellDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LSTMEncoder(dim, dim)\n",
    "# dim*2 because encoder is bidirectional\n",
    "decoder = LSTMCellDecoder(dim, dim*2, len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# criterion = nn.NLLLoss()\n",
    "# encoder_optim = torch.optim.Adam()\n",
    "# decoder_optim = torch.optim.Adam()\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     epoch_loss = 0\n",
    "#     for x, y in data_loader:\n",
    "#         _, (contexts, _) = encoder(x)\n",
    "#         outputs = decoder(contexts)\n",
    "        \n",
    "#         y = pad_sequence(y, outputs.shape[0])\n",
    "        \n",
    "#         loss = criterion(outputs, y)\n",
    "#         loss.backward()\n",
    "        \n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item() / len(x)\n",
    "        \n",
    "#     print(f'epoch_loss={epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_out, (e_hidden, _) = encoder(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.cat((e_hidden[0], e_hidden[1]), dim=1).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out, d_hidden = decoder(defs, decoder_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_loss ="
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: For loss function... instead of doing log_softmax, do MSE with actual GloVe vector and minimize this loss function.\n",
    "Then for BLEU evaluation, you'll need a function to find the closest vector to the one produced by the model.\n",
    "\n",
    "Interesting to compare these results to log_softmax"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "146.989px",
    "width": "305px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}