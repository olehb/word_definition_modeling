{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import Embeddings, load_glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils import squash_packed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Determining device to run on"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading hyperparameters from SageMaker env variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.4769, -0.0846,  1.4641,  0.0470,  0.1469,  0.5082, -1.2228, -0.2261,\n         0.1931, -0.2976,  0.2060, -0.7128, -1.6288,  0.1710,  0.7480, -0.0619,\n        -0.6577,  1.3786, -0.6804, -1.7551,  0.5832,  0.2516, -1.2114,  0.8134,\n         0.0948, -1.6819, -0.6450,  0.6322,  1.1211,  0.1611,  2.5379,  0.2485,\n        -0.2682,  0.3282,  1.2916,  0.2355,  0.6147, -0.1344, -0.1324,  0.2740,\n        -0.1182,  0.1354,  0.0743, -0.6195,  0.4547, -0.3032, -0.2188, -0.5605,\n         1.1177, -0.3659])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = int(os.environ.get('SM_HP_GLOVE_DIM', 50))\n",
    "max_length = int(os.environ.get('SM_HP_MAX_LENGTH', 30))\n",
    "data_loc = os.environ.get('SM_HP_DATA_LOC', '../data')\n",
    "epochs = int(os.environ.get('SM_HP_EPOCHS', 2))\n",
    "batch = int(os.environ.get('SM_HP_BATCH', 32))\n",
    "lr = float(os.environ.get('SM_HP_LR', 0.01))\n",
    "train_remotely = bool(int(os.environ.get('SM_HP_TRAIN_REMOTELY', 1)))\n",
    "is_sagemaker_estimator = 'TRAINING_JOB_NAME' in os.environ  # This code is running on the remote SageMaker estimator machine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initializing GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "embeddings = load_glove_embeddings(dim, data_loc)\n",
    "len(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "embeddings['car']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initializing data loaders for Oxford2019 dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:12<00:00,  2.49s/it]\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train loss 64.1812973022461, val loss N/A\n",
      "Epoch 1 | train loss 63.40849590301514, val loss N/A\n"
     ]
    }
   ],
   "source": [
    "from dataset import Oxford2019Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def make_data_loader(filename: str, file_loc: str = os.path.join(data_loc, 'Oxford-2019')) -> DataLoader:\n",
    "    dataset = Oxford2019Dataset(data_loc=os.path.join(file_loc, filename))\n",
    "    data_loader = DataLoader(dataset, batch_size=batch, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "train_set = make_data_loader('train.txt')\n",
    "test_set = make_data_loader('test.txt')\n",
    "valid_set = make_data_loader('valid.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility functions to work with PackedSequences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import List, Iterable, Callable\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, PackedSequence\n",
    "\n",
    "def sentence_to_list(sent: str) -> Iterable[str]:\n",
    "    return chain([Embeddings.SOS_STR], sent.split(), [Embeddings.EOS_STR])\n",
    "\n",
    "def to_packed_sequence(tensors: List[torch.Tensor]) -> PackedSequence:\n",
    "    lens = [len(t) for t in tensors]\n",
    "    packed = pack_padded_sequence(pad_sequence(tensors), lens, enforce_sorted=False).to(device)\n",
    "    return packed\n",
    "\n",
    "def strings_to_batch(strings: List[str]) -> PackedSequence:\n",
    "    sents_emb = [embeddings.sentence_to_tensor(sentence_to_list(sent)) for sent in strings]\n",
    "    return to_packed_sequence(sents_emb)\n",
    "\n",
    "def strings_to_ids(strings: List[str]) -> PackedSequence:\n",
    "    ids = [embeddings.sentence_to_ids(sentence_to_list(sent)) for sent in strings]\n",
    "    return to_packed_sequence(ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to run through one epoch\n",
    "This function is used in training, validation, and testing phases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run(encoder: nn.Module,\n",
    "        decoder: nn.Module,\n",
    "        data_loader: DataLoader,\n",
    "        criterion: nn.NLLLoss,\n",
    "        post_hook: Callable = lambda b: ''):\n",
    "    log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    loss = 0\n",
    "    for words, defs, examples in tqdm(data_loader, disable=is_sagemaker_estimator):\n",
    "        word_first_examples = (''.join((w, e)) for w, e in zip(words, examples))\n",
    "\n",
    "        examples_ps = strings_to_batch(word_first_examples)\n",
    "        defs_ps = strings_to_batch(defs)\n",
    "        def_ids = strings_to_ids(defs)\n",
    "\n",
    "        e_out, e_hidden = encoder(examples_ps)\n",
    "\n",
    "        decoder_input = torch.cat((e_hidden[0], e_hidden[1]), dim=1)\n",
    "        # decoder_input = decoder_input[examples.unsorted_indices]  # pytorch will do this automatically\n",
    "        # decoder_input = decoder_input[defs.sorted_indices]  # pytorch will do this automatically\n",
    "        decoder_input = decoder_input.unsqueeze(dim=0)\n",
    "\n",
    "        d_out = decoder(defs_ps, decoder_input)\n",
    "        d_out_lsm = squash_packed(d_out, log_softmax).data\n",
    "\n",
    "        batch_loss = criterion(d_out_lsm, def_ids.data)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        post_hook(batch_loss)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loop function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from wdm import LSTMEncoder, LSTMCellDecoder\n",
    "\n",
    "def train(epochs: int, train_data_loader: DataLoader, valid_data_loader: DataLoader = None):\n",
    "    encoder = LSTMEncoder(dim, dim).to(device)\n",
    "    # dim*2 because encoder is bidirectional\n",
    "    decoder = LSTMCellDecoder(dim, dim*2, len(embeddings)).to(device)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    encoder_optim = torch.optim.Adam(encoder.parameters())\n",
    "    decoder_optim = torch.optim.Adam(decoder.parameters())\n",
    "\n",
    "    def update_weights(batch_loss):\n",
    "        batch_loss.backward()\n",
    "\n",
    "        encoder_optim.step()\n",
    "        decoder_optim.step()\n",
    "\n",
    "        encoder_optim.zero_grad()\n",
    "        decoder_optim.zero_grad()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = run(encoder, decoder, train_data_loader, criterion, update_weights)\n",
    "\n",
    "        if valid_data_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                encoder.eval()\n",
    "                decoder.eval()\n",
    "                val_loss = run(encoder, decoder, valid_data_loader, criterion)\n",
    "        else:\n",
    "            val_loss = 'N/A'\n",
    "\n",
    "        msg = f'Epoch {i} | train loss {train_loss}, val loss {val_loss}'\n",
    "        print(msg)\n",
    "        !echo '{msg}' >> log.txt\n",
    "    return encoder, decoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quick sanity check for the training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_file = os.path.join(data_loc, 'Oxford-2019', 'train.txt')\n",
    "tiny_size = batch * 5\n",
    "tiny_file = os.path.join(data_loc, 'Oxford-2019', 'tiny.txt')\n",
    "!head -n {tiny_size} {train_file} > {tiny_file}\n",
    "tiny_set = make_data_loader('tiny.txt')\n",
    "encoder, decoder = train(epochs=2, train_data_loader=tiny_set, valid_data_loader=tiny_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function for saving the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def save_model(encoder, decoder):\n",
    "    out_loc = '/opt/ml/model' if is_sagemaker_estimator else '.'\n",
    "    !mkdir -p {out_loc}\n",
    "    \n",
    "    with open(os.path.join(out_loc, 'encoder.pt'), 'wb') as f:\n",
    "        torch.save(encoder.state_dict(), f)\n",
    "\n",
    "    with open(os.path.join(out_loc, 'decoder.pt'), 'wb') as f:\n",
    "        torch.save(decoder.state_dict(), f)\n",
    "\n",
    "    !cp main.py {out_loc}\n",
    "    !cp main.ipynb {out_loc}\n",
    "    !cp log.txt {out_loc}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "Training can be done either on the same machine where notebook is running or remotely on SageMaker estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "if is_sagemaker_estimator:\n",
    "    encoder, decoder = train(epochs=epochs, train_data_loader=train_set, valid_data_loader=valid_set)\n",
    "    save_model(encoder, decoder)\n",
    "elif train_remotely:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    output_path = f's3://chegg-ds-data/oboiko/wdm-output'\n",
    "\n",
    "    pytorch_estimator = PyTorch(entry_point='train.sh',\n",
    "                                base_job_name='wdm-1',\n",
    "                                role=role,\n",
    "                                train_instance_count=1,\n",
    "                                train_instance_type='ml.g4dn.2xlarge',  # GPU instance\n",
    "                                train_volume_size=50,\n",
    "                                train_max_run=86400,  # 24 hours\n",
    "                                hyperparameters={\n",
    "                                  'glove_dim': 50,\n",
    "                                  'max_length': 30,\n",
    "                                  'data_loc': '/opt/data',\n",
    "                                  'batch': 50,\n",
    "                                  'epochs': 2,\n",
    "                                  'lr': 0.01,\n",
    "                                  'train_remotely': 0\n",
    "                                },\n",
    "                                framework_version='1.6.0',\n",
    "                                py_version='py3',\n",
    "                                source_dir='.',  # This entire folder will be transferred to training instance\n",
    "                                debugger_hook_config=False,\n",
    "                                output_path=output_path,  # Model files will be uploaded here\n",
    "                                image_name='954558792927.dkr.ecr.us-west-2.amazonaws.com/sagemaker/wdm:latest'\n",
    "                     )\n",
    "\n",
    "    pytorch_estimator.fit('s3://chegg-ds-data/oboiko/wdm/dummy.txt', wait=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: For loss function... instead of doing log_softmax, do MSE with actual GloVe vector and minimize this loss function.\n",
    "Then for BLEU evaluation, you'll need a function to find the closest vector to the one produced by the model.\n",
    "\n",
    "Interesting to compare these results to log_softmax"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "146.989px",
    "width": "305px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}