{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_glove_embeddings(dim)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1392,  0.7532, -0.1715, -0.6622, -0.1074,  0.3893, -0.4517, -0.7152,\n",
       "        -0.3411,  0.9171,  0.5661,  0.2525,  1.0873,  0.9969,  0.1705,  0.8662,\n",
       "         0.0614, -0.1375, -0.2858, -1.2783,  0.6066, -0.3275,  0.7814, -0.7399,\n",
       "         0.1129, -0.3309, -1.3136,  1.4563,  0.0447, -1.1595,  0.3302, -0.5312,\n",
       "        -0.6305,  0.0097, -0.5013, -0.0992,  0.1624,  0.9057,  0.7850, -0.1771,\n",
       "         0.7864, -0.1800,  0.1099, -0.1697,  0.5408,  0.3937,  0.2866, -0.6887,\n",
       "        -0.3778, -1.0411])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['wax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"sequencer\", \"dna sequencing was carried out using the dye termination kit and an automatic sequencer\", \"an apparatus for determining the sequence of amino acids or other monomers in a biological polymer\"),\n",
    "    (\"order\", \"the templars were also known as the order of christ\", \"a society of knights bound by a common rule of life and having a combined military and monastic character\"),\n",
    "    (\"horde\", \"an army or tribe of nomadic warriors\", \"the viking hordes returned to york this weekend as fierce armoured warriors mingled with the city centre crowds .\"),\n",
    "    (\"anaemic\", \"although it has been thought of as a symptom of iron deficiency , it is more commonly discovered in patients who are not anemic .\", \"suffering from anaemia\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import (pad_sequence, \n",
    "                                pack_padded_sequence, \n",
    "                                pad_packed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = sorted([d[1].split() for d in data], key=lambda s: len(s), reverse=True)\n",
    "sents_emb = [embeddings.sentence_to_tensor(sent) for sent in sents]\n",
    "lens = [len(s) for s in sents_emb]\n",
    "batch = pack_padded_sequence(pad_sequence(sents_emb), lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wdm import LSTMEncoder, LSTMCellDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LSTMEncoder(dim, dim)\n",
    "# dim*2 because encoder is bidirectional\n",
    "decoder = LSTMCellDecoder(max_length, dim*2, len(embeddings))\n",
    "data_loader = init_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# criterion = nn.NLLLoss()\n",
    "# encoder_optim = torch.optim.Adam()\n",
    "# decoder_optim = torch.optim.Adam()\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     epoch_loss = 0\n",
    "#     for x, y in data_loader:\n",
    "#         _, (contexts, _) = encoder(x)\n",
    "#         outputs = decoder(contexts)\n",
    "        \n",
    "#         y = pad_sequence(y, outputs.shape[0])\n",
    "        \n",
    "#         loss = criterion(outputs, y)\n",
    "#         loss.backward()\n",
    "        \n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item() / len(x)\n",
    "        \n",
    "#     print(f'epoch_loss={epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (hidden, _) = encoder(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.cat((hidden[0], hidden[1]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oboiko/dev/S/word_definition_modeling/wdm.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.relu(out)\n"
     ]
    }
   ],
   "source": [
    "out = decoder(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: For loss function... instead of doing log_softmax, do MSE with actual GloVe vector and minimize this loss function.\n",
    "Then for BLEU evaluation, you'll need a function to find the closest vector to the one produced by the model.\n",
    "\n",
    "Interesting to compare these results to log_softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "146.989px",
    "width": "305px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
