{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import Embeddings, load_glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "400002"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_glove_embeddings(dim)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings['car']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "dataset = Oxford2019Dataset()\n",
    "dataset_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=10, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"sequencer\", \"dna sequencing was carried out using the dye termination kit and an automatic sequencer\", \"an apparatus for determining the sequence of amino acids or other monomers in a biological polymer\"),\n",
    "    (\"order\", \"the templars were also known as the order of christ\", \"a society of knights bound by a common rule of life and having a combined military and monastic character\"),\n",
    "    (\"horde\", \"an army or tribe of nomadic warriors\", \"the viking hordes returned to york this weekend as fierce armoured warriors mingled with the city centre crowds .\"),\n",
    "    (\"anaemic\", \"although it has been thought of as a symptom of iron deficiency , it is more commonly discovered in patients who are not anemic .\", \"suffering from anaemia\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import (pad_sequence, \n",
    "                                pack_padded_sequence, \n",
    "                                pad_packed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def to_packed_sequence(items: List[torch.Tensor]):\n",
    "    lens = [len(s) for s in items]\n",
    "    return pack_padded_sequence(pad_sequence(items), lens, enforce_sorted=False)\n",
    "\n",
    "def sentence_to_list(sent):\n",
    "    return [Embeddings.SOS_STR] + sent.split() + [Embeddings.EOS_STR]\n",
    "\n",
    "def strings_to_batch(strings: List[str]) -> torch.nn.utils.rnn.PackedSequence:\n",
    "    sents = (sentence_to_list(sent) for sent in strings)\n",
    "    sents_emb = [embeddings.sentence_to_tensor(sent) for sent in sents]\n",
    "    lens = [len(s) for s in sents_emb]\n",
    "    batch = pack_padded_sequence(pad_sequence(sents_emb), lens, enforce_sorted=False)\n",
    "    return batch, lens\n",
    "\n",
    "def strings_to_ids(strings: List[str]) -> List[int]:\n",
    "    ids = [embeddings.sentence_to_ids(sentence_to_list(sent)) for sent in strings]\n",
    "    return to_packed_sequence(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "examples, examples_lens = strings_to_batch([d[1] for d in data])\n",
    "defs, defs_lens = strings_to_batch([d[2] for d in data])\n",
    "defs_ids = strings_to_ids([d[2] for d in data])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wdm import LSTMEncoder, LSTMCellDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LSTMEncoder(dim, dim)\n",
    "# dim*2 because encoder is bidirectional\n",
    "decoder = LSTMCellDecoder(dim, dim*2, len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'marshbird'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-115-9ea4095371e6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mwords\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexamples\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdataset_loader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mexamples_ps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexamples_lens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstrings_to_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mexamples\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0mdefs_ps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefs_lens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstrings_to_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdefs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0mdef_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstrings_to_ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdefs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-82-e8fc51050abd>\u001B[0m in \u001B[0;36mstrings_to_batch\u001B[0;34m(strings)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mstrings_to_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstrings\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPackedSequence\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0msents\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msentence_to_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstrings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0msents_emb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msentence_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msents\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0mlens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msents_emb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpack_padded_sequence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpad_sequence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msents_emb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menforce_sorted\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-82-e8fc51050abd>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mstrings_to_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstrings\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPackedSequence\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0msents\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msentence_to_list\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mstrings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0msents_emb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msentence_to_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msent\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msent\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msents\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0mlens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msents_emb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpack_padded_sequence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpad_sequence\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msents_emb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menforce_sorted\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Users/oboiko/dev/S/word_definition_modeling/embeddings.py\u001B[0m in \u001B[0;36msentence_to_tensor\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m     18\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m             \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'marshbird'"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters())\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters())\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for words, defs, examples in dataset_loader:\n",
    "        examples_ps, examples_lens = strings_to_batch(examples)\n",
    "        defs_ps, defs_lens = strings_to_batch(defs)\n",
    "        def_ids = strings_to_ids(defs)\n",
    "\n",
    "#     for x, y in data_loader:\n",
    "#         _, (contexts, _) = encoder(x)\n",
    "#         outputs = decoder(contexts)\n",
    "        \n",
    "#         y = pad_sequence(y, outputs.shape[0])\n",
    "        \n",
    "#         loss = criterion(outputs, y)\n",
    "#         loss.backward()\n",
    "        \n",
    "#         encoder_optimizer.step()\n",
    "#         decoder_optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item() / len(x)\n",
    "        \n",
    "#     print(f'epoch_loss={epoch_loss})\n",
    "# from utils import squash_packed\n",
    "# criterion = nn.NLLLoss()\n",
    "# log_softmax = nn.LogSoftmax(dim=1)\n",
    "# loss = criterion(squash_packed(d_out, log_softmax).data, defs_ids.data)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_out, e_hidden = encoder(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.cat((e_hidden[0], e_hidden[1]), dim=1)\n",
    "# decoder_input = decoder_input[examples.unsorted_indices]  # pytorch will do this automatically\n",
    "# decoder_input = decoder_input[defs.sorted_indices]  # pytorch will do this automatically\n",
    "decoder_input = decoder_input.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = decoder(defs, decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.8941, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-94-56a328f645d0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Users/oboiko/opt/miniconda3/envs/S37/lib/python3.7/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    183\u001B[0m                 \u001B[0mproducts\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mDefaults\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    184\u001B[0m         \"\"\"\n\u001B[0;32m--> 185\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    186\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Users/oboiko/opt/miniconda3/envs/S37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    125\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m    126\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: For loss function... instead of doing log_softmax, do MSE with actual GloVe vector and minimize this loss function.\n",
    "Then for BLEU evaluation, you'll need a function to find the closest vector to the one produced by the model.\n",
    "\n",
    "Interesting to compare these results to log_softmax"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "146.989px",
    "width": "305px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}