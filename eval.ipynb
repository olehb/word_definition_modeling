{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from wdm import LSTMEncoder, LSTMCellDecoder\n",
    "from embeddings import load_glove_embeddings, Embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dim = 50\n",
    "data_loc = '../data'\n",
    "model_loc = 'model'\n",
    "batch = 1\n",
    "max_length = 30"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "400003"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = load_glove_embeddings(dim, data_loc)\n",
    "len(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_encoder(filename):\n",
    "    encoder = LSTMEncoder(dim, dim)\n",
    "    with open(filename, 'rb') as f:\n",
    "        encoder.load_state_dict(torch.load(f, map_location=torch.device('cpu')))\n",
    "    return encoder\n",
    "\n",
    "def load_decoder(filename):\n",
    "    decoder = LSTMCellDecoder(dim, dim*2, len(embeddings))\n",
    "    with open(filename, 'rb') as f:\n",
    "        decoder.load_state_dict(torch.load(f, map_location=torch.device('cpu')))\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "encoder = load_encoder(os.path.join(model_loc, 'encoder.pt'))\n",
    "decoder = load_decoder(os.path.join(model_loc, 'decoder.pt'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import List, Iterable\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, PackedSequence\n",
    "\n",
    "def sentence_to_list(sent: str) -> Iterable[str]:\n",
    "    return chain([Embeddings.SOS_STR], sent.split(), [Embeddings.EOS_STR])\n",
    "\n",
    "def to_packed_sequence(tensors: List[torch.Tensor]) -> PackedSequence:\n",
    "    lens = [len(t) for t in tensors]\n",
    "    packed = pack_padded_sequence(pad_sequence(tensors), lens, enforce_sorted=False).to(device)\n",
    "    return packed\n",
    "\n",
    "def strings_to_batch(strings: List[str]) -> PackedSequence:\n",
    "    sents_emb = [embeddings.sentence_to_tensor(sentence_to_list(sent)) for sent in strings]\n",
    "    return to_packed_sequence(sents_emb)\n",
    "\n",
    "def strings_to_ids(strings: List[str]) -> PackedSequence:\n",
    "    ids = [embeddings.sentence_to_ids(sentence_to_list(sent)) for sent in strings]\n",
    "    return to_packed_sequence(ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from dataset import Oxford2019Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def make_data_loader(filename: str, file_loc: str = os.path.join(data_loc, 'Oxford-2019')) -> DataLoader:\n",
    "    dataset = Oxford2019Dataset(data_loc=os.path.join(file_loc, filename))\n",
    "    data_loader = DataLoader(dataset, batch_size=batch, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "test_set = make_data_loader('test.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "from torch.nn import LogSoftmax, Softmax\n",
    "from random import randint\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "lsm = LogSoftmax(dim=1)\n",
    "sm = Softmax(dim=1)\n",
    "\n",
    "\n",
    "def generate_text(word, example):\n",
    "    word_first_example = ' '.join((word, example))\n",
    "\n",
    "    encoder_input = embeddings.sentence_to_tensor(sentence_to_list(word_first_example))\n",
    "    encoder_input = encoder_input.unsqueeze(dim=1)\n",
    "    e_out, e_hidden = encoder(encoder_input)\n",
    "\n",
    "    decoder_input = torch.cat((e_hidden[0], e_hidden[1]), dim=1)\n",
    "    decoder_input = decoder_input.unsqueeze(dim=0)\n",
    "\n",
    "    result = [Embeddings.SOS_STR]\n",
    "    while result[-1] != embeddings.EOS_STR and len(result) <= max_length:\n",
    "        token_emb = embeddings[result[-1]]\n",
    "        d_out, decoder_input = decoder(token_emb.view(1, 1, -1), decoder_input)\n",
    "        sort = torch.argsort(sm(d_out))\n",
    "        id = sort[0][0][randint(0, 5)]\n",
    "        id = sort[0][0][0]\n",
    "        token = embeddings.id2word[id.item()]\n",
    "        result.append(token)\n",
    "    return ' '.join(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "{'vespers': '<s> kissane termly termly eleniak termly kissane woundwort bawean 5,430 termly eleniak bawean 5,430 kissane bawean termly eleniak eleniak 5,430 bawean bawean woundwort woundwort termly eleniak bawean eleniak kissane kissane eleniak',\n 'ancestor': '<s> bawean woundwort termly termly 5,430 woundwort 5,430 eleniak eleniak termly kissane kissane termly bawean eleniak 5,430 5,430 bawean 5,430 kissane eleniak termly kissane bawean 5,430 bawean termly eleniak kissane kissane',\n 'monitor': '<s> eleniak termly bawean 5,430 5,430 5,430 eleniak termly woundwort eleniak 5,430 eleniak eleniak termly kissane bawean 5,430 kissane termly termly termly 5,430 bawean kissane 5,430 bawean kissane eleniak termly termly',\n 'canvasback': '<s> 5,430 termly eleniak eleniak 5,430 bawean 5,430 bawean bawean 5,430 bawean termly 5,430 eleniak 5,430 woundwort kissane 5,430 woundwort termly eleniak woundwort woundwort bawean bawean eleniak bawean kissane bawean woundwort',\n 'skimmer': '<s> kissane woundwort 5,430 termly bawean 5,430 eleniak termly bawean 5,430 termly termly 5,430 kissane eleniak kissane 5,430 5,430 termly woundwort eleniak kissane kissane 5,430 termly woundwort termly termly kissane eleniak',\n 'freeze': '<s> kissane termly termly bawean termly termly eleniak bawean bawean bawean bawean termly eleniak 5,430 termly bawean 5,430 kissane eleniak 5,430 eleniak bawean kissane eleniak kissane 5,430 bawean eleniak bawean 5,430'}"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = {}\n",
    "with torch.no_grad():\n",
    "    for words, defs, examples in test_set:\n",
    "        for word, definition, example in zip(words, defs, examples):\n",
    "            text = generate_text(word, example)\n",
    "            all_results[word] = text\n",
    "\n",
    "        if len(all_results) > 5:\n",
    "            break\n",
    "all_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "'<s> bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean bawean'"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"guildhall\", \"from 1709 until the early nineteenth century the goldsmiths' company had their guildhall in werburgh street , close to dublin castle\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}